for testing, remember to set DELTA in test_command.sh


run on 10 different nodes:
okeanos-login3 /home/balawend> srun -n 10 --ntasks-per-node=1 --time=00:00:15 --account=g101-2284 hostname
nid00124
nid00126
nid00122
nid00127
nid00119
nid00123
nid00125
nid00118
nid00120
nid00121


okeanos-login3 /home/balawend> scontrol show partition okeanos
PartitionName=okeanos
   AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
   AllocNodes=ALL Default=YES QoS=N/A
   DefaultTime=02:00:00 DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO
   MaxNodes=UNLIMITED MaxTime=14-00:00:00 MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED
   Nodes=nid0[0008-0062,0072-0127,0136-0190,0192-0383,0392-0446,0456-0511,0520-0574,0576-0767,0776-0831,0840-0895,0900-1151]
   PriorityJobFactor=10 PriorityTier=10 RootOnly=NO ReqResv=NO OverSubscribe=EXCLUSIVE
   OverTimeLimit=NONE PreemptMode=OFF
   State=UP TotalCPUs=51840 TotalNodes=1080 SelectTypeParameters=NONE
   JobDefaults=(null)
   DefMemPerNode=UNLIMITED MaxMemPerNode=UNLIMITED


okeanos-login3 /home/balawend> sacctmgr show qos format=Name,MaxNodes,MaxJobs,MaxWall
      Name MaxNodes MaxJobs     MaxWall 
---------- -------- ------- ----------- 
    normal                              
       hpc                   2-00:00:00 
  internal                   2-00:00:00 
     meteo                   2-00:00:00 
      long                   7-00:00:00 

ReservationName=mimuww StartTime=2025-06-11T16:00:00 EndTime=2025-06-11T18:00:00 Duration=02:00:00
   Nodes=nid000[50-55] NodeCnt=6 CoreCnt=144 Features=(null) PartitionName=(null) Flags=WEEKLY,SPEC_NODES
   TRES=cpu=288
   Users= Groups=(null) Accounts=g101-2284 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a
   MaxStartDelay=(null)

ReservationName=mimuwp StartTime=2025-06-13T16:00:00 EndTime=2025-06-13T18:00:00 Duration=02:00:00
   Nodes=nid000[50-55] NodeCnt=6 CoreCnt=144 Features=(null) PartitionName=(null) Flags=WEEKLY,SPEC_NODES
   TRES=cpu=288
   Users= Groups=(null) Accounts=g101-2284 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a
   MaxStartDelay=(null)



Odpalanie:
[balawend@hpc ~/distributed-sssp]$ make test
[balawend@hpc ~/distributed-sssp]$ ssh okeanos
okeanos-login3 /home/balawend> cd testing_env/
okeanos-login3 balawend/testing_env> sbatch sbatch_run_tests.sh
okeanos-login3 balawend/testing_env> squeue | grep balawend
okeanos-login3 balawend/testing_env> cat output.txt

okeanos-login3 balawend/testing_env> srun -n 10 ./test_command.sh LOGIN69 path_20_4


# max nodes is 90!


check disk quota:
lfs quota -uh $USER /lu/tetyda/home/


Graph500:

ta biblioteka zwraca 48 bitów per node-id.
ale tylko <SCALE> dolnych bitów jest ważna!

dla scale=9, powinno byc 512 nodeow.
node_ids = sorted(set(int.from_bytes(x, byteorder='little') & 0x1ff  for e in es for x in e))
len(set(node_ids)) == 294
jest mniej, bo niektore wierzcholki nie dostaly zadnej krawedzi po prostu


zmienna: setenv TESTMAX 30000


NOTE: duze testy mi dają OOM, bo puszczam zbyt duzo procesow na jednym nodzie!

rozmiar testu bigcycle_1000 000 000 _ 48:
20.84 miliona krawędzi per proces (E)
20.84 miliona wierzcholkow (V)

size(distToRoot) = sizeof(long long) * V = 8 * |V|
size(neighOfLocal) = Sum_{e in E} 2 * (sizeof(size_t) + sizeof(long long)) = 32 * |E|

czyli rozmiar:
21M * 8 = 200MB na distToRoot
21M * 32 = 800MB na neighOfLocal
powinno wejść!

a bigtest na 5B?
5GB per proces.

odpalanie: setenv TESTMAX 100000 ; make test-big


1c5a12744f98f331304966fe6d82eb0505e8cde9 - ten commit ma wpisana moj opt, ale wczesniejszy przechodzi na pewno
spoko testy

64567e75a1b44aa146b195e97ac20022915f3e8a - ten commit wygląda sensownie dla bigcycle


ZLICZANIE:
- number of phases, dla roznych DELTA, dla skal 28-30
- number of relaxations, dla roznych DELTA, skale 28-30